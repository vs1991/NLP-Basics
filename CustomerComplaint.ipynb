{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CustomerComplaint.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNOnRFEsjnNdrsgN4e02M90",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vs1991/NLP-Basics/blob/main/CustomerComplaint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmkIsFjlnB5l",
        "outputId": "8d5c028f-77fd-4025-ce02-2b4bdfa63f3f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIZTF-FnnLnA"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Data Science -NLP\"\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUieKVrFnXX1",
        "outputId": "8320a84c-0b03-43b2-cf82-365e9b83dd3e"
      },
      "source": [
        "#changing the working directory\n",
        "%cd /content/gdrive/My Drive/Data Science -NLP\n",
        "#Check the present working directory using pwd command"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Data Science -NLP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggkJqZqOnnz4",
        "outputId": "8dd4295d-3c10-4772-c6f4-d64dcae729f1"
      },
      "source": [
        "ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Author Iden Test.csv'  'Author Iden Train.csv'   Consumer_Complaints.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpQ_95FRoB6o"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "ZyQ9ggshnoUU",
        "outputId": "012abc5e-8470-4698-b449-1e57b73bb669"
      },
      "source": [
        "full_data=pd.read_csv('Consumer_Complaints.csv')\n",
        "full_data.head(3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date received</th>\n",
              "      <th>Product</th>\n",
              "      <th>Sub-product</th>\n",
              "      <th>Issue</th>\n",
              "      <th>Sub-issue</th>\n",
              "      <th>Consumer complaint narrative</th>\n",
              "      <th>Company public response</th>\n",
              "      <th>Company</th>\n",
              "      <th>State</th>\n",
              "      <th>ZIP code</th>\n",
              "      <th>Tags</th>\n",
              "      <th>Consumer consent provided?</th>\n",
              "      <th>Submitted via</th>\n",
              "      <th>Date sent to company</th>\n",
              "      <th>Company response to consumer</th>\n",
              "      <th>Timely response?</th>\n",
              "      <th>Consumer disputed?</th>\n",
              "      <th>Complaint ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3/12/2014</td>\n",
              "      <td>Mortgage</td>\n",
              "      <td>Other mortgage</td>\n",
              "      <td>Loan modification,collection,foreclosure</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>M&amp;T BANK CORPORATION</td>\n",
              "      <td>MI</td>\n",
              "      <td>48382</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Referral</td>\n",
              "      <td>3/17/2014</td>\n",
              "      <td>Closed with explanation</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>759217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10/1/2016</td>\n",
              "      <td>Credit reporting</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Incorrect information on credit report</td>\n",
              "      <td>Account status</td>\n",
              "      <td>I have outdated information on my credit repor...</td>\n",
              "      <td>Company has responded to the consumer and the ...</td>\n",
              "      <td>TRANSUNION INTERMEDIATE HOLDINGS, INC.</td>\n",
              "      <td>AL</td>\n",
              "      <td>352XX</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Consent provided</td>\n",
              "      <td>Web</td>\n",
              "      <td>10/5/2016</td>\n",
              "      <td>Closed with explanation</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>2141773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10/17/2016</td>\n",
              "      <td>Consumer Loan</td>\n",
              "      <td>Vehicle loan</td>\n",
              "      <td>Managing the loan or lease</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I purchased a new car on XXXX XXXX. The car de...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CITIZENS FINANCIAL GROUP, INC.</td>\n",
              "      <td>PA</td>\n",
              "      <td>177XX</td>\n",
              "      <td>Older American</td>\n",
              "      <td>Consent provided</td>\n",
              "      <td>Web</td>\n",
              "      <td>10/20/2016</td>\n",
              "      <td>Closed with explanation</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>2163100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Date received           Product  ... Consumer disputed? Complaint ID\n",
              "0     3/12/2014          Mortgage  ...                 No       759217\n",
              "1     10/1/2016  Credit reporting  ...                 No      2141773\n",
              "2    10/17/2016     Consumer Loan  ...                 No      2163100\n",
              "\n",
              "[3 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZNBECVOoAwM",
        "outputId": "b5358f82-79da-4c10-f649-ba527b228bf3"
      },
      "source": [
        "print(full_data.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(903983, 18)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lONlbfDoS0t"
      },
      "source": [
        "Since our data contains 903983 complaints,I will consider only consider only few complaints in our data analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru_vnFudp2W6"
      },
      "source": [
        "**Our goal is to send the complaints to the right vertical**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "R7XkhYFUoRAn",
        "outputId": "cbfd970f-7d31-40da-a2f0-3b185183663e"
      },
      "source": [
        "data=full_data[['Consumer complaint narrative','Product']][0:20000]\n",
        "data.dropna(inplace=True)\n",
        "data.columns=['X','y']\n",
        "data.head(3)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I have outdated information on my credit repor...</td>\n",
              "      <td>Credit reporting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I purchased a new car on XXXX XXXX. The car de...</td>\n",
              "      <td>Consumer Loan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>An account on my credit report has a mistaken ...</td>\n",
              "      <td>Credit reporting</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   X                 y\n",
              "1  I have outdated information on my credit repor...  Credit reporting\n",
              "2  I purchased a new car on XXXX XXXX. The car de...     Consumer Loan\n",
              "7  An account on my credit report has a mistaken ...  Credit reporting"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0znCALH8JGtp",
        "outputId": "286efd73-61dd-4095-a16b-6d9ac83ce273"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4218, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKcluKrRp9DC",
        "outputId": "9d7e846b-6395-4b4c-f455-b1cdba6fd16b"
      },
      "source": [
        "print(data.iloc[2][0])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "An account on my credit report has a mistaken date. I mailed in a debt validation letter to allow XXXX to correct the information. I received a letter in the mail, stating that Experian received my correspondence and found it to be \" suspicious '' and that \" I did n't write it ''. Experian 's letter is worded to imply that I am incapable of writing my own letter. I was deeply offended by this implication. \n",
            "I called Experian to figure out why my letter was so suspicious. I spoke to a representative who was incredibly unhelpful, She did not effectively answer any questions I asked of her, and she kept ignoring what I was saying regarding the offensive letter and my dispute process. I feel the representative did what she wanted to do, and I am not satisfied. It is STILL not clear to me why I received this letter. I typed this letter, I signed this letter, and I paid to mail this letter, yet Experian willfully disregarded my lawful request. \n",
            "I am disgusted with this entire situation, and I would like for my dispute to be handled appropriately, and I would like for an Experian representative to contact me and give me a real explanation for this letter.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrrYxgZ8q2sL"
      },
      "source": [
        "**Word combine to from sentence ,sentence combine to form paragraphs.**\n",
        "First,we need to convert the text data(customer complaint narrative)into numerical data,since ML model can't read text data.**How do we do that ?**\n",
        "\n",
        "Here comes NLP models.Before that let's go through some of the difficulty of using text data.\n",
        "\n",
        "\n",
        "\n",
        "1.   **Synonymy:-**   different words having the same meaning\n",
        "2.   **Ambiguity :-** The bank deposit rate is quite high\" and \"He stood near the bank admiring the river\". In these statements, the word bank has completely different meanings.\n",
        "3.   **Anaphora Resolution:-**George is my friend. He likes football\". In the second statement he refers to George. It is difficult for the computers to discern what person/entity the pronoun he is referring to.\n",
        "4.   **Language related issues :-**Every language has its own uniqueness\n",
        "5.   **Out of Vocabulary problem:-**Machines have a hard time adapting to any new constructs that humans come up with. As humans when we come across a word we haven't seen earlier, we might not understand its meaning instantly. But this does not mean we cannot adapt. After looking at the word in several different sentences and understanding its usage, we understand the context and meaning of the new word. Machines can only handle data that they have seen before. It is unable to adapt well.\n",
        "6.   **Language generation:-**While language understanding is hard, language generation too has its own set of challenges. For chatbots to work effectively, they need to communication properly constructed sentences which are grammatically correct. This is quite a hard problem and a challenge that needs to be overcome. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVJh1vdhtnoR"
      },
      "source": [
        "**Use cases of NLP**\n",
        "\n",
        "1. **Sentiment Analysis -** Finding if the text is leaning towards a positive or negative sentiment.\n",
        "2. **Text Classification -** Categorizing text to various categories\n",
        "3. **Document Summarization -** Compressing a paragraph/document into few words or sentences\n",
        "4. **Parts of Speech Tagging-** Figuring out the various nouns, adverbs, verbs etc in the text\n",
        "5.  **Machine translation -** Translate text from one language to another\n",
        "6. **Named Entity Recognition -** Identify the entities present in text\n",
        "7. **Conversational AI -** Chat with a machine in natural language and get queries resolved\n",
        "\n",
        "\n",
        "To answer our question how to convert text data into numerical data \n",
        "***For that, we need to arrive at a fundamental component of text known as tokens.***\n",
        "\n",
        "\n",
        "Tokenization is the process of splitting the text into smaller parts called tokens. Tokens are the basic units of a particular dataset. The choice of tokens could be based on the application we are working on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcMA7zCkza72",
        "outputId": "d0ef96b9-5337-4670-927f-8832c7762012"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ectkfzwmqDv6",
        "outputId": "d4a03b73-a7e4-4b19-ac69-80e887d25cf0"
      },
      "source": [
        "#text pre-processing library\n",
        "#first step is to breakdown sentences into words ,we will use the nltk library for that \n",
        "import nltk \n",
        "from nltk.tokenize import word_tokenize \n",
        "# we will drop na values from the data \n",
        "#data.dropna(inplace=True)\n",
        "print('New Shape of data is ',data.shape,'\\n')\n",
        "print('let us look at our first complaint:-')\n",
        "first_complaint=data.iloc[5][0]\n",
        "print(first_complaint)\n",
        "\n",
        "#let us tokenzie the data using split function of pandas \n",
        "bag_of_words_split=first_complaint.split()\n",
        "print(bag_of_words_split,'\\n')\n",
        "\n",
        "#let us tokenzie the data using nltk word tokenize \n",
        "bag_of_words_wordt=word_tokenize(first_complaint)\n",
        "print(bag_of_words_wordt,'\\n')\n",
        "print(first_complaint[23])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New Shape of data is  (4218, 2) \n",
            "\n",
            "let us look at our first complaint:-\n",
            "Started the refinance of home mortgage process with cash out option on XX/XX/2016. Necessary documents were submitted by XXXX. After initial review, got good faith estimate with loan amount and closing cost. Based on this estimate, a deposit of {$350.00} was made towards appraisal. Appraisal came with lesser amount by {$5000.00}. Agreed to reduce the loan amount to that extent. However, got a revised estimate which was less by {$30000.00} and with additional closing cost towards points etc. In between got numerous revised estimates with different loan amounts and closing cost. It took more than 2 months to reach any definite closing document. Hence, want to get back the deposit of {$350.00}.\n",
            "['Started', 'the', 'refinance', 'of', 'home', 'mortgage', 'process', 'with', 'cash', 'out', 'option', 'on', 'XX/XX/2016.', 'Necessary', 'documents', 'were', 'submitted', 'by', 'XXXX.', 'After', 'initial', 'review,', 'got', 'good', 'faith', 'estimate', 'with', 'loan', 'amount', 'and', 'closing', 'cost.', 'Based', 'on', 'this', 'estimate,', 'a', 'deposit', 'of', '{$350.00}', 'was', 'made', 'towards', 'appraisal.', 'Appraisal', 'came', 'with', 'lesser', 'amount', 'by', '{$5000.00}.', 'Agreed', 'to', 'reduce', 'the', 'loan', 'amount', 'to', 'that', 'extent.', 'However,', 'got', 'a', 'revised', 'estimate', 'which', 'was', 'less', 'by', '{$30000.00}', 'and', 'with', 'additional', 'closing', 'cost', 'towards', 'points', 'etc.', 'In', 'between', 'got', 'numerous', 'revised', 'estimates', 'with', 'different', 'loan', 'amounts', 'and', 'closing', 'cost.', 'It', 'took', 'more', 'than', '2', 'months', 'to', 'reach', 'any', 'definite', 'closing', 'document.', 'Hence,', 'want', 'to', 'get', 'back', 'the', 'deposit', 'of', '{$350.00}.'] \n",
            "\n",
            "['Started', 'the', 'refinance', 'of', 'home', 'mortgage', 'process', 'with', 'cash', 'out', 'option', 'on', 'XX/XX/2016', '.', 'Necessary', 'documents', 'were', 'submitted', 'by', 'XXXX', '.', 'After', 'initial', 'review', ',', 'got', 'good', 'faith', 'estimate', 'with', 'loan', 'amount', 'and', 'closing', 'cost', '.', 'Based', 'on', 'this', 'estimate', ',', 'a', 'deposit', 'of', '{', '$', '350.00', '}', 'was', 'made', 'towards', 'appraisal', '.', 'Appraisal', 'came', 'with', 'lesser', 'amount', 'by', '{', '$', '5000.00', '}', '.', 'Agreed', 'to', 'reduce', 'the', 'loan', 'amount', 'to', 'that', 'extent', '.', 'However', ',', 'got', 'a', 'revised', 'estimate', 'which', 'was', 'less', 'by', '{', '$', '30000.00', '}', 'and', 'with', 'additional', 'closing', 'cost', 'towards', 'points', 'etc', '.', 'In', 'between', 'got', 'numerous', 'revised', 'estimates', 'with', 'different', 'loan', 'amounts', 'and', 'closing', 'cost', '.', 'It', 'took', 'more', 'than', '2', 'months', 'to', 'reach', 'any', 'definite', 'closing', 'document', '.', 'Hence', ',', 'want', 'to', 'get', 'back', 'the', 'deposit', 'of', '{', '$', '350.00', '}', '.'] \n",
            "\n",
            "f\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA9orb3x0x4n"
      },
      "source": [
        "One could also tokenize a paragraph into constituent sentences. To split into definite sentences\n",
        "\n",
        "The importance of converting words to lower case - All words should be converted to lowercase while doing NLP. The reason behind being, that \"Yorkshire\" and \"yorkshire\" even though are the same word, will be considered 2 separate words while converting the words into numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0w5gvrwx-PG",
        "outputId": "0f78ef2b-6feb-499b-97d8-3cfee0f66196"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "#sentence tokenizing using sent_tokenize\n",
        "list_of_sentences=sent_tokenize(first_complaint)\n",
        "print(list_of_sentences,'\\n')\n",
        "\n",
        "#lowering first complaint \n",
        "first_complaint_lower=first_complaint.lower()\n",
        "print(first_complaint_lower,'\\n')\n",
        "\n",
        "#worde tokenizing the lower case complaints \n",
        "bag_of_words_lower=word_tokenize(first_complaint_lower)\n",
        "print(bag_of_words_lower)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Started the refinance of home mortgage process with cash out option on XX/XX/2016.', 'Necessary documents were submitted by XXXX.', 'After initial review, got good faith estimate with loan amount and closing cost.', 'Based on this estimate, a deposit of {$350.00} was made towards appraisal.', 'Appraisal came with lesser amount by {$5000.00}.', 'Agreed to reduce the loan amount to that extent.', 'However, got a revised estimate which was less by {$30000.00} and with additional closing cost towards points etc.', 'In between got numerous revised estimates with different loan amounts and closing cost.', 'It took more than 2 months to reach any definite closing document.', 'Hence, want to get back the deposit of {$350.00}.'] \n",
            "\n",
            "started the refinance of home mortgage process with cash out option on xx/xx/2016. necessary documents were submitted by xxxx. after initial review, got good faith estimate with loan amount and closing cost. based on this estimate, a deposit of {$350.00} was made towards appraisal. appraisal came with lesser amount by {$5000.00}. agreed to reduce the loan amount to that extent. however, got a revised estimate which was less by {$30000.00} and with additional closing cost towards points etc. in between got numerous revised estimates with different loan amounts and closing cost. it took more than 2 months to reach any definite closing document. hence, want to get back the deposit of {$350.00}. \n",
            "\n",
            "['started', 'the', 'refinance', 'of', 'home', 'mortgage', 'process', 'with', 'cash', 'out', 'option', 'on', 'xx/xx/2016', '.', 'necessary', 'documents', 'were', 'submitted', 'by', 'xxxx', '.', 'after', 'initial', 'review', ',', 'got', 'good', 'faith', 'estimate', 'with', 'loan', 'amount', 'and', 'closing', 'cost', '.', 'based', 'on', 'this', 'estimate', ',', 'a', 'deposit', 'of', '{', '$', '350.00', '}', 'was', 'made', 'towards', 'appraisal', '.', 'appraisal', 'came', 'with', 'lesser', 'amount', 'by', '{', '$', '5000.00', '}', '.', 'agreed', 'to', 'reduce', 'the', 'loan', 'amount', 'to', 'that', 'extent', '.', 'however', ',', 'got', 'a', 'revised', 'estimate', 'which', 'was', 'less', 'by', '{', '$', '30000.00', '}', 'and', 'with', 'additional', 'closing', 'cost', 'towards', 'points', 'etc', '.', 'in', 'between', 'got', 'numerous', 'revised', 'estimates', 'with', 'different', 'loan', 'amounts', 'and', 'closing', 'cost', '.', 'it', 'took', 'more', 'than', '2', 'months', 'to', 'reach', 'any', 'definite', 'closing', 'document', '.', 'hence', ',', 'want', 'to', 'get', 'back', 'the', 'deposit', 'of', '{', '$', '350.00', '}', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9XiNRlNMHYR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSWD0uvRMH9D"
      },
      "source": [
        "# Stemming and Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l-Z90Y92wwI"
      },
      "source": [
        "**Stemming** Stemming is the process of converting the words of a sentence to its non-changing portions. So stemming a word or sentence may result in words that are not actual words. Stems are created by removing the suffixes or prefixes used with a word.\n",
        "\n",
        "For eg: Likes, liked, likely, unlike⇒like\n",
        "\n",
        " The popular ones include:\n",
        "\n",
        "1. Porter Stemmer(Implemented in almost all languages)\n",
        "2. Paice Stemmer\n",
        "3. Lovins Stemmer\n",
        "\n",
        "\n",
        "**Lemmatization:**\n",
        "\n",
        "This method is a more refined way of breaking words through the use of a vocabulary and morphological analysis of words. The aim is to always return the base form of a word known as lemma.\n",
        "\n",
        "Consider the following words:\n",
        "\n",
        "'Studied', 'Studious' ,'Studying'\n",
        "\n",
        "Stemming of them will result in Studi\n",
        "\n",
        "Lemmatisation of them will result in Stu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BpVYSSUELRi"
      },
      "source": [
        "**Bag of words:**\n",
        "\n",
        "The problem with modeling text is that there is no well defined fixed-length inputs.\n",
        "\n",
        "A bag of words model is a way of extracting features from text for use in modeling. In this approach, we use the tokenized words for each observation and find out the frequency of each token.\n",
        "\n",
        "This process of converting text data to numbers is called vectorization\n",
        "\n",
        "There are multiple methods to convert words to numbers. We will be start with discussing the count Vectorizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueX1UTLS1Spb",
        "outputId": "c5690f64-6ae1-419b-c17d-b3036874cea7"
      },
      "source": [
        "from collections import Counter\n",
        "count_vectorizer=Counter(bag_of_words_lower)\n",
        "print(count_vectorizer)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'.': 10, 'with': 5, ',': 4, 'closing': 4, '{': 4, '$': 4, '}': 4, 'to': 4, 'the': 3, 'of': 3, 'by': 3, 'got': 3, 'estimate': 3, 'loan': 3, 'amount': 3, 'and': 3, 'cost': 3, 'on': 2, 'a': 2, 'deposit': 2, '350.00': 2, 'was': 2, 'towards': 2, 'appraisal': 2, 'revised': 2, 'started': 1, 'refinance': 1, 'home': 1, 'mortgage': 1, 'process': 1, 'cash': 1, 'out': 1, 'option': 1, 'xx/xx/2016': 1, 'necessary': 1, 'documents': 1, 'were': 1, 'submitted': 1, 'xxxx': 1, 'after': 1, 'initial': 1, 'review': 1, 'good': 1, 'faith': 1, 'based': 1, 'this': 1, 'made': 1, 'came': 1, 'lesser': 1, '5000.00': 1, 'agreed': 1, 'reduce': 1, 'that': 1, 'extent': 1, 'however': 1, 'which': 1, 'less': 1, '30000.00': 1, 'additional': 1, 'points': 1, 'etc': 1, 'in': 1, 'between': 1, 'numerous': 1, 'estimates': 1, 'different': 1, 'amounts': 1, 'it': 1, 'took': 1, 'more': 1, 'than': 1, '2': 1, 'months': 1, 'reach': 1, 'any': 1, 'definite': 1, 'document': 1, 'hence': 1, 'want': 1, 'get': 1, 'back': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqZZg21UEjB2",
        "outputId": "d755de22-d18a-4ff1-df7c-acd7d96fcae1"
      },
      "source": [
        "from sklearn.metrics import accuracy_score,roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#Code starts here\n",
        "\n",
        "#Subsetting 'X'\n",
        "all_text = data[[\"X\"]]\n",
        "\n",
        "#Converting 'X' to lower case\n",
        "all_text[\"X\"] = all_text['X'].str.lower()\n",
        "\n",
        "#Initialising a count vectorizer object\n",
        "cv = CountVectorizer()\n",
        "\n",
        "#Creating the count vectorizer of our 'X' column\n",
        "vector =cv.fit_transform(all_text[\"X\"])\n",
        "\n",
        "#Converting the count vectoriser to array\n",
        "X = vector.toarray()\n",
        "\n",
        "#Subsetting y\n",
        "labels = data[[\"y\"]]\n",
        "\n",
        "#Initialising a label encoder object\n",
        "le = LabelEncoder()\n",
        "\n",
        "#Label encoding 'y' column\n",
        "labels[\"y\"] = le.fit_transform(labels[\"y\"])\n",
        "\n",
        "#Splitting the dataset into train and test\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,labels[\"y\"],test_size=0.4,random_state=42)\n",
        "\n",
        "#Initialising Logistic Regression model\n",
        "log_reg = LogisticRegression(random_state=42)\n",
        "\n",
        "#Fitting the model on train data\n",
        "log_reg.fit(X_train,y_train)\n",
        "\n",
        "#Finding the accuracy score on test data\n",
        "acc = log_reg.score(X_test,y_test)\n",
        "print ('Accuracy with stopwords',acc)\n",
        "\n",
        "#Code ends here"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy with stopwords 0.7292654028436019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2PCX-kEKTrl"
      },
      "source": [
        "**Removing Stopwords**\n",
        "\n",
        "\n",
        "we have seen 49% accuracy of predicting the product category. Now the question we need to ask is - can we improve the accuracy further? The answer lies in dealing with stopwords.\n",
        "\n",
        "\n",
        "**Stop Words**\n",
        "\n",
        "count vectorizer is essentially a Ranking algorithm, in the way that it gives a higher weight to words which have appeared more number of times. \n",
        " It does not assign any importance to the order in which the words have appeared in the sentences.\n",
        "Another point to be considered is the fact that words like \"a\",\"an\",\"the\" etc. will appear more number of times than the rest of the words as they are common articles. Using a Count vectorizer out of the box on a paragraph or a body of text will invariably give the highest count to these common words.\n",
        "\n",
        "**One way to rectify this, is to remove these commonly occurring words.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAl4XTMTJso2",
        "outputId": "60252456-ae8f-4527-eba8-dfe74cef4f5e"
      },
      "source": [
        "cv_stop=CountVectorizer(stop_words='english')\n",
        "\n",
        "#creating a count vectorizer of our x column\n",
        "vector_stop=cv_stop.fit_transform(all_text['X'])\n",
        "\n",
        "#converting the count vectorizer to array \n",
        "X_stop=vector_stop.toarray()\n",
        "\n",
        "#Splitting the data to train and test\n",
        "X_train,X_test,y_train,y_test = train_test_split(X_stop,labels[\"y\"],test_size=0.4,random_state=42)\n",
        "\n",
        "#Initalising a logistic regression model\n",
        "log_reg = LogisticRegression(random_state=42)\n",
        "\n",
        "#Fitting the model on train\n",
        "log_reg.fit(X_train,y_train)\n",
        "\n",
        "#Finding the accuracy score on test data\n",
        "stop_acc = log_reg.score(X_test,y_test)\n",
        "print ('Accuracy without stopwords',stop_acc)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy without stopwords 0.7470379146919431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Dq3pUdGMC3G"
      },
      "source": [
        "\n",
        "a count vectorizer, counts the occurences of the words in a document and all the documents are considered independent of each other.However in cases where multiple documents are involved, count vectorizer still does not assume any interdependence between the documents and considers each of the documents as a seperate entity.\n",
        "\n",
        "\n",
        "**It does not rank the words based on their importance in the document, but just based on whether they exist or not.**\n",
        "\n",
        "In fact, the process of converting, text to numbers should essentially be a ranking system of the words so that the documents can each get a score based on what words they contain. All words cannot have the same imprtance or relevance in the document right?\n",
        "There are two ways to approach document similarity:\n",
        "\n",
        "1. TF-IDF Score\n",
        "\n",
        "2. Cosine Similarity\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOvkh6qVNImU"
      },
      "source": [
        "# 1. Tfidf(Term Frequency and Inverse Document Frequency)  with examples\n",
        "is the holy grail of ranking metrics to convert text to numbers \n",
        "\n",
        "\n",
        "\n",
        "**The ranking system in a count vectorizer is purely occurence based on a single document only!**\n",
        "\n",
        "TF-IDF takes it a step further and ranks the words based not just on their occurences in one document but across all the documents.\n",
        "if CV or Count vectorizer was giving more importance to words because they have appeared multiple times in the document, TF-IDF will rank them high if they have appeared only in that document, meaning that they are rare, hence higher importance and lower if they have appeared in all or most documents, because they are more common, hence lower ranking.\n",
        "\n",
        "Consider a scenario where there are 5 documents and all are talking about football. The word football would have appeared multiple times in each document. CV is going to rank football consistently high and infact give the word football a different value across all 5 documents based on how many times that word has appeared in that document. In other words, it is assuming, that the more number of times a word appears, the more important it is. That is exactly what the TF or the Term Frequency component in TF-IDF does.\n",
        "\n",
        "**IDF** on the other hand now is the dominating factor in TFIDF which is going to find out the number of times football has also appeared in the other 4 documents except for the one it is currently seeing. If football has also appeared in rest of the documents, it means that though football is important to that one document based on the number of occurences, considering it has appeared in the rest as well, it is not that rare or more common, hence the importance now is going to reduce instead of going high! italicised text\n",
        "\n",
        "\n",
        "**The ranking system is across the entire corpus or all documents. It is not a single document based metric!**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTbtEqv3OxFB"
      },
      "source": [
        "**How is Tfidf calculate?**\n",
        "\n",
        "\n",
        "the tfidf constitues of 2 parts :\n",
        "\n",
        "the 1st computes the normalized **Term Frequency(Tf)** a.k.a the no. of times the word has appeared in a document,divided by the total no. of words in the document.\n",
        "\n",
        "the 2nd term **Inverse Document Frequency(IDF)** computes the logarithm of the no. of the documents to the number of times the word appears in the different documents.\n",
        "\n",
        "\n",
        "**TF:** Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:\n",
        "\n",
        "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
        "\n",
        "\n",
        "**IDF:** Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:\n",
        "\n",
        "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JK-byHbhLDg0",
        "outputId": "a4390fd6-97be-4bca-e0dc-4cd486e8fbaa"
      },
      "source": [
        "complaint_1=data['X'].iloc[0]\n",
        "complaint_2=data['X'].iloc[1]\n",
        "complaint_3=data['X'].iloc[2]\n",
        "\n",
        "print (\"Complaint 1: \", complaint_1)\n",
        "\n",
        "print (\"\\nComplaint 2: \", complaint_2)\n",
        "\n",
        "print (\"\\nComplaint 3: \", complaint_3)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Complaint 1:  I have outdated information on my credit report that I have previously disputed that has yet to be removed this information is more then seven years old and does not meet credit reporting requirements\n",
            "\n",
            "Complaint 2:  I purchased a new car on XXXX XXXX. The car dealer called Citizens Bank to get a 10 day payoff on my loan, good till XXXX XXXX. The dealer sent the check the next day. When I balanced my checkbook on XXXX XXXX. I noticed that Citizens bank had taken the automatic payment out of my checking account at XXXX XXXX XXXX Bank. I called Citizens and they stated that they did not close the loan until XXXX XXXX. ( stating that they did not receive the check until XXXX. XXXX. ). I told them that I did not believe that the check took that long to arrive. XXXX told me a check was issued to me for the amount overpaid, they deducted additional interest. Today ( XXXX XXXX, ) I called Citizens Bank again and talked to a supervisor named XXXX, because on XXXX XXXX. I received a letter that the loan had been paid in full ( dated XXXX, XXXX ) but no refund check was included. XXXX stated that they hold any over payment for 10 business days after the loan was satisfied and that my check would be mailed out on Wed. the XX/XX/XXXX.. I questioned her about the delay in posting the dealer payment and she first stated that sometimes it takes 3 or 4 business days to post, then she said they did not receive the check till XXXX XXXX I again told her that I did not believe this and asked where is my money. She then stated that they hold the over payment for 10 business days. I asked her why, and she simply said that is their policy. I asked her if I would receive interest on my money and she stated no. I believe that Citizens bank is deliberately delaying the posting of payment and the return of consumer 's money to make additional interest for the bank. If this is not illegal it should be, it does hurt the consumer and is not ethical. My amount of money lost is minimal but if they are doing this on thousands of car loans a month, then the additional interest earned for them could be staggering. I still have another car loan from Citizens Bank and I am afraid when I trade that car in another year I will run into the same problem again.\n",
            "\n",
            "Complaint 3:  An account on my credit report has a mistaken date. I mailed in a debt validation letter to allow XXXX to correct the information. I received a letter in the mail, stating that Experian received my correspondence and found it to be \" suspicious '' and that \" I did n't write it ''. Experian 's letter is worded to imply that I am incapable of writing my own letter. I was deeply offended by this implication. \n",
            "I called Experian to figure out why my letter was so suspicious. I spoke to a representative who was incredibly unhelpful, She did not effectively answer any questions I asked of her, and she kept ignoring what I was saying regarding the offensive letter and my dispute process. I feel the representative did what she wanted to do, and I am not satisfied. It is STILL not clear to me why I received this letter. I typed this letter, I signed this letter, and I paid to mail this letter, yet Experian willfully disregarded my lawful request. \n",
            "I am disgusted with this entire situation, and I would like for my dispute to be handled appropriately, and I would like for an Experian representative to contact me and give me a real explanation for this letter.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNKEm2dwQsXi",
        "outputId": "35cd4297-5a3e-4ad0-e5ea-a91f12f4c5ac"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# list of text documents called sents\n",
        "sents = [complaint_1, complaint_2, complaint_3]\n",
        "# create the transform\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "vectorizer.fit(sents)\n",
        "\n",
        "vector = vectorizer.transform(sents)\n",
        "\n",
        "print(\"Shape of the vectorized sentence:\",vector.shape)\n",
        "\n",
        "vector_values = vector.toarray().tolist()[0]\n",
        "\n",
        "print(\"The tf-idf score of first five elements:\",vector_values[:20])\n",
        "\n",
        "\n",
        "# Converting the tf-idf score with the word into a dictionary\n",
        "import operator\n",
        "sorted_x=sorted(vectorizer.vocabulary_.items(),key=operator.itemgetter(1))\n",
        "\n",
        "words=[x[0] for x in sorted_x]\n",
        "d=dict(zip(words,vector_values))\n",
        "print(\"Dictionary of words with tf-idf score:\\n\", d)\n",
        "print(\"Sorted dictonary:\\n\")\n",
        "print (sorted(d.items(), key=operator.itemgetter(1), reverse = True))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the vectorized sentence: (3, 223)\n",
            "The tf-idf score of first five elements: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11807902273813668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Dictionary of words with tf-idf score:\n",
            " {'10': 0.0, 'about': 0.0, 'account': 0.0, 'additional': 0.0, 'afraid': 0.0, 'after': 0.0, 'again': 0.0, 'allow': 0.0, 'am': 0.0, 'amount': 0.0, 'an': 0.0, 'and': 0.11807902273813668, 'another': 0.0, 'answer': 0.0, 'any': 0.0, 'appropriately': 0.0, 'are': 0.0, 'arrive': 0.0, 'asked': 0.0, 'at': 0.0, 'automatic': 0.0, 'balanced': 0.0, 'bank': 0.0, 'be': 0.11807902273813668, 'because': 0.0, 'been': 0.0, 'believe': 0.0, 'business': 0.0, 'but': 0.0, 'by': 0.0, 'called': 0.0, 'car': 0.0, 'check': 0.0, 'checkbook': 0.0, 'checking': 0.0, 'citizens': 0.0, 'clear': 0.0, 'close': 0.0, 'consumer': 0.0, 'contact': 0.0, 'correct': 0.0, 'correspondence': 0.0, 'could': 0.0, 'credit': 0.3040964814250496, 'date': 0.0, 'dated': 0.0, 'day': 0.0, 'days': 0.0, 'dealer': 0.0, 'debt': 0.0, 'deducted': 0.0, 'deeply': 0.0, 'delay': 0.0, 'delaying': 0.0, 'deliberately': 0.0, 'did': 0.0, 'disgusted': 0.0, 'dispute': 0.0, 'disputed': 0.1999251644323498, 'disregarded': 0.0, 'do': 0.0, 'does': 0.1520482407125248, 'doing': 0.0, 'earned': 0.0, 'effectively': 0.0, 'entire': 0.0, 'ethical': 0.0, 'experian': 0.0, 'explanation': 0.0, 'feel': 0.0, 'figure': 0.0, 'first': 0.0, 'for': 0.0, 'found': 0.0, 'from': 0.0, 'full': 0.0, 'get': 0.0, 'give': 0.0, 'good': 0.0, 'had': 0.0, 'handled': 0.0, 'has': 0.1520482407125248, 'have': 0.3040964814250496, 'her': 0.0, 'hold': 0.0, 'hurt': 0.0, 'if': 0.0, 'ignoring': 0.0, 'illegal': 0.0, 'implication': 0.0, 'imply': 0.0, 'in': 0.0, 'incapable': 0.0, 'included': 0.0, 'incredibly': 0.0, 'information': 0.3040964814250496, 'interest': 0.0, 'into': 0.0, 'is': 0.11807902273813668, 'issued': 0.0, 'it': 0.0, 'kept': 0.0, 'lawful': 0.0, 'letter': 0.0, 'like': 0.0, 'loan': 0.0, 'loans': 0.0, 'long': 0.0, 'lost': 0.0, 'mail': 0.0, 'mailed': 0.0, 'make': 0.0, 'me': 0.0, 'meet': 0.1999251644323498, 'minimal': 0.0, 'mistaken': 0.0, 'money': 0.0, 'month': 0.0, 'more': 0.1999251644323498, 'my': 0.11807902273813668, 'named': 0.0, 'new': 0.0, 'next': 0.0, 'no': 0.0, 'not': 0.11807902273813668, 'noticed': 0.0, 'of': 0.0, 'offended': 0.0, 'offensive': 0.0, 'old': 0.1999251644323498, 'on': 0.11807902273813668, 'or': 0.0, 'out': 0.0, 'outdated': 0.1999251644323498, 'over': 0.0, 'overpaid': 0.0, 'own': 0.0, 'paid': 0.0, 'payment': 0.0, 'payoff': 0.0, 'policy': 0.0, 'post': 0.0, 'posting': 0.0, 'previously': 0.1999251644323498, 'problem': 0.0, 'process': 0.0, 'purchased': 0.0, 'questioned': 0.0, 'questions': 0.0, 'real': 0.0, 'receive': 0.0, 'received': 0.0, 'refund': 0.0, 'regarding': 0.0, 'removed': 0.1999251644323498, 'report': 0.1520482407125248, 'reporting': 0.1999251644323498, 'representative': 0.0, 'request': 0.0, 'requirements': 0.1999251644323498, 'return': 0.0, 'run': 0.0, 'said': 0.0, 'same': 0.0, 'satisfied': 0.0, 'saying': 0.0, 'sent': 0.0, 'seven': 0.1999251644323498, 'she': 0.0, 'should': 0.0, 'signed': 0.0, 'simply': 0.0, 'situation': 0.0, 'so': 0.0, 'sometimes': 0.0, 'spoke': 0.0, 'staggering': 0.0, 'stated': 0.0, 'stating': 0.0, 'still': 0.0, 'supervisor': 0.0, 'suspicious': 0.0, 'taken': 0.0, 'takes': 0.0, 'talked': 0.0, 'that': 0.23615804547627336, 'the': 0.0, 'their': 0.0, 'them': 0.0, 'then': 0.1520482407125248, 'they': 0.0, 'this': 0.11807902273813668, 'thousands': 0.0, 'till': 0.0, 'to': 0.11807902273813668, 'today': 0.0, 'told': 0.0, 'took': 0.0, 'trade': 0.0, 'typed': 0.0, 'unhelpful': 0.0, 'until': 0.0, 'validation': 0.0, 'wanted': 0.0, 'was': 0.0, 'wed': 0.0, 'what': 0.0, 'when': 0.0, 'where': 0.0, 'who': 0.0, 'why': 0.0, 'will': 0.0, 'willfully': 0.0, 'with': 0.0, 'worded': 0.0, 'would': 0.0, 'write': 0.0, 'writing': 0.0, 'xx': 0.0, 'xxxx': 0.0, 'year': 0.0, 'years': 0.1999251644323498, 'yet': 0.1520482407125248}\n",
            "Sorted dictonary:\n",
            "\n",
            "[('credit', 0.3040964814250496), ('have', 0.3040964814250496), ('information', 0.3040964814250496), ('that', 0.23615804547627336), ('disputed', 0.1999251644323498), ('meet', 0.1999251644323498), ('more', 0.1999251644323498), ('old', 0.1999251644323498), ('outdated', 0.1999251644323498), ('previously', 0.1999251644323498), ('removed', 0.1999251644323498), ('reporting', 0.1999251644323498), ('requirements', 0.1999251644323498), ('seven', 0.1999251644323498), ('years', 0.1999251644323498), ('does', 0.1520482407125248), ('has', 0.1520482407125248), ('report', 0.1520482407125248), ('then', 0.1520482407125248), ('yet', 0.1520482407125248), ('and', 0.11807902273813668), ('be', 0.11807902273813668), ('is', 0.11807902273813668), ('my', 0.11807902273813668), ('not', 0.11807902273813668), ('on', 0.11807902273813668), ('this', 0.11807902273813668), ('to', 0.11807902273813668), ('10', 0.0), ('about', 0.0), ('account', 0.0), ('additional', 0.0), ('afraid', 0.0), ('after', 0.0), ('again', 0.0), ('allow', 0.0), ('am', 0.0), ('amount', 0.0), ('an', 0.0), ('another', 0.0), ('answer', 0.0), ('any', 0.0), ('appropriately', 0.0), ('are', 0.0), ('arrive', 0.0), ('asked', 0.0), ('at', 0.0), ('automatic', 0.0), ('balanced', 0.0), ('bank', 0.0), ('because', 0.0), ('been', 0.0), ('believe', 0.0), ('business', 0.0), ('but', 0.0), ('by', 0.0), ('called', 0.0), ('car', 0.0), ('check', 0.0), ('checkbook', 0.0), ('checking', 0.0), ('citizens', 0.0), ('clear', 0.0), ('close', 0.0), ('consumer', 0.0), ('contact', 0.0), ('correct', 0.0), ('correspondence', 0.0), ('could', 0.0), ('date', 0.0), ('dated', 0.0), ('day', 0.0), ('days', 0.0), ('dealer', 0.0), ('debt', 0.0), ('deducted', 0.0), ('deeply', 0.0), ('delay', 0.0), ('delaying', 0.0), ('deliberately', 0.0), ('did', 0.0), ('disgusted', 0.0), ('dispute', 0.0), ('disregarded', 0.0), ('do', 0.0), ('doing', 0.0), ('earned', 0.0), ('effectively', 0.0), ('entire', 0.0), ('ethical', 0.0), ('experian', 0.0), ('explanation', 0.0), ('feel', 0.0), ('figure', 0.0), ('first', 0.0), ('for', 0.0), ('found', 0.0), ('from', 0.0), ('full', 0.0), ('get', 0.0), ('give', 0.0), ('good', 0.0), ('had', 0.0), ('handled', 0.0), ('her', 0.0), ('hold', 0.0), ('hurt', 0.0), ('if', 0.0), ('ignoring', 0.0), ('illegal', 0.0), ('implication', 0.0), ('imply', 0.0), ('in', 0.0), ('incapable', 0.0), ('included', 0.0), ('incredibly', 0.0), ('interest', 0.0), ('into', 0.0), ('issued', 0.0), ('it', 0.0), ('kept', 0.0), ('lawful', 0.0), ('letter', 0.0), ('like', 0.0), ('loan', 0.0), ('loans', 0.0), ('long', 0.0), ('lost', 0.0), ('mail', 0.0), ('mailed', 0.0), ('make', 0.0), ('me', 0.0), ('minimal', 0.0), ('mistaken', 0.0), ('money', 0.0), ('month', 0.0), ('named', 0.0), ('new', 0.0), ('next', 0.0), ('no', 0.0), ('noticed', 0.0), ('of', 0.0), ('offended', 0.0), ('offensive', 0.0), ('or', 0.0), ('out', 0.0), ('over', 0.0), ('overpaid', 0.0), ('own', 0.0), ('paid', 0.0), ('payment', 0.0), ('payoff', 0.0), ('policy', 0.0), ('post', 0.0), ('posting', 0.0), ('problem', 0.0), ('process', 0.0), ('purchased', 0.0), ('questioned', 0.0), ('questions', 0.0), ('real', 0.0), ('receive', 0.0), ('received', 0.0), ('refund', 0.0), ('regarding', 0.0), ('representative', 0.0), ('request', 0.0), ('return', 0.0), ('run', 0.0), ('said', 0.0), ('same', 0.0), ('satisfied', 0.0), ('saying', 0.0), ('sent', 0.0), ('she', 0.0), ('should', 0.0), ('signed', 0.0), ('simply', 0.0), ('situation', 0.0), ('so', 0.0), ('sometimes', 0.0), ('spoke', 0.0), ('staggering', 0.0), ('stated', 0.0), ('stating', 0.0), ('still', 0.0), ('supervisor', 0.0), ('suspicious', 0.0), ('taken', 0.0), ('takes', 0.0), ('talked', 0.0), ('the', 0.0), ('their', 0.0), ('them', 0.0), ('they', 0.0), ('thousands', 0.0), ('till', 0.0), ('today', 0.0), ('told', 0.0), ('took', 0.0), ('trade', 0.0), ('typed', 0.0), ('unhelpful', 0.0), ('until', 0.0), ('validation', 0.0), ('wanted', 0.0), ('was', 0.0), ('wed', 0.0), ('what', 0.0), ('when', 0.0), ('where', 0.0), ('who', 0.0), ('why', 0.0), ('will', 0.0), ('willfully', 0.0), ('with', 0.0), ('worded', 0.0), ('would', 0.0), ('write', 0.0), ('writing', 0.0), ('xx', 0.0), ('xxxx', 0.0), ('year', 0.0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpkHzn-PREUu",
        "outputId": "e18d133d-51ff-4d79-ec98-7c782bbd9e30"
      },
      "source": [
        "sents=[]\n",
        "for x in range(100):\n",
        "    sents.append(data[\"X\"].iloc[x])\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# list of text documents called sents\n",
        "# create the transform\n",
        "vectorizer = TfidfVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(sents)\n",
        "vector = vectorizer.transform(sents)\n",
        "vector.shape \n",
        "vector_values = vector.toarray().tolist()[0]\n",
        "\n",
        "sorted_x = sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))\n",
        "words = [x[0] for x in sorted_x]\n",
        "d = dict(zip(words,vector_values))\n",
        "print(\"Sorted dictionary: \\n\")\n",
        "print ((sorted(d.items(), key=operator.itemgetter(1), reverse = True))[:20])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sorted dictionary: \n",
            "\n",
            "[('outdated', 0.30698816696178366), ('requirements', 0.30698816696178366), ('seven', 0.28169892181541933), ('meet', 0.26375591683431887), ('information', 0.25693994145225296), ('previously', 0.2498382409130902), ('disputed', 0.23846667168795455), ('old', 0.22052366670685408), ('yet', 0.21317742654159022), ('credit', 0.20755516271607583), ('does', 0.19523442156048976), ('more', 0.19523442156048976), ('removed', 0.19024208161296743), ('reporting', 0.1856198967748199), ('have', 0.1669613523183918), ('then', 0.15742915392599566), ('that', 0.15697667242334717), ('years', 0.15465665513790783), ('report', 0.1340591664519245), ('has', 0.1302779510221311)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5eRoWIMSaym"
      },
      "source": [
        "# Tfidf On our dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yQbiutUSSXU",
        "outputId": "7c2dbb2e-1589-4fe9-ce0c-2f7efae596e5"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf=TfidfVectorizer(stop_words='english')\n",
        "\n",
        "vector_tfidf=tfidf.fit_transform(data['X'])\n",
        "\n",
        "#converting this to array \n",
        "X_tfidf=vector_tfidf.toarray()\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X_tfidf,labels[\"y\"],test_size=0.4,random_state=42)\n",
        "\n",
        "#Initalising a logistic regression model\n",
        "log_reg = LogisticRegression(random_state=42)\n",
        "\n",
        "#Fitting the model on train\n",
        "log_reg.fit(X_train,y_train)\n",
        "\n",
        "#Finding the accuracy score on test data\n",
        "tfidf_acc = log_reg.score(X_test,y_test)\n",
        "print ('Accuracy with Tfidf Vectorizer',tfidf_acc)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy with Tfidf Vectorizer 0.7571090047393365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTQHx8hNW4sf"
      },
      "source": [
        "# Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycV8g_k6Tg7I",
        "outputId": "0dc699ab-055b-4f9d-9779-89003e76641a"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "all_text = data[[\"X\"]]\n",
        "\n",
        "# Converting the 'X' column to lower case\n",
        "all_text[\"X\"] = all_text['X'].str.lower()\n",
        "\n",
        "# Initialising a tfidf vectorizer object with stopwords\n",
        "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
        "\n",
        "# Vectorizing the 'X' column\n",
        "vector = tfidf.fit_transform(all_text[\"X\"])\n",
        "\n",
        "# Converting vector to array\n",
        "X_tfidf = vector.toarray()\n",
        "\n",
        "# y\n",
        "\n",
        "# Subsetting 'y' column\n",
        "labels = data[[\"y\"]]\n",
        "\n",
        "# Initialising label encoder object\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Label encoding 'y' column\n",
        "labels[\"y\"] = le.fit_transform(labels[\"y\"])\n",
        "\n",
        "# Splitting the data into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, labels[\"y\"], test_size=0.4, random_state=42)\n",
        "\n",
        "# Initialsing a naive bayes classifier\n",
        "nb = MultinomialNB()\n",
        "\n",
        "# Fitting the model on train data\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Finding the accuracy score of model on test data\n",
        "nb_acc = nb.score(X_test, y_test)\n",
        "print(nb_acc)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6143364928909952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FSHeYfMVf_b",
        "outputId": "deccb6dc-6552-47b8-e24f-79f07619198b"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print (classification_report(y_test,nb.predict(X_test)))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.16      0.28       154\n",
            "           1       0.00      0.00      0.00       107\n",
            "           2       0.72      0.32      0.44       193\n",
            "           3       0.76      0.80      0.78       310\n",
            "           4       0.50      0.94      0.66       397\n",
            "           5       0.00      0.00      0.00        22\n",
            "           6       0.64      0.91      0.75       344\n",
            "           7       0.00      0.00      0.00         3\n",
            "           8       0.00      0.00      0.00        21\n",
            "           9       0.00      0.00      0.00        16\n",
            "          10       0.93      0.12      0.21       121\n",
            "\n",
            "    accuracy                           0.61      1688\n",
            "   macro avg       0.41      0.30      0.28      1688\n",
            "weighted avg       0.63      0.61      0.54      1688\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vnsq88sqXC-l",
        "outputId": "580db599-b391-4d65-91bc-67a9b99c8cae"
      },
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "ros=RandomOverSampler(random_state=42)\n",
        "X_ros,y_ros=ros.fit_sample(X_train,y_train)\n",
        "nb = MultinomialNB()\n",
        "# Fitting the model on train data\n",
        "nb.fit(X_ros, y_ros)\n",
        "\n",
        "# Finding the accuracy score of model on test data\n",
        "nb_acc = nb.score(X_test, y_test)\n",
        "print(nb_acc)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.773696682464455\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yftf__BDX3rt",
        "outputId": "06b35515-43ea-44ee-98b9-c705cfe1cd0c"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print (classification_report(y_test,nb.predict(X_test)))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.80      0.73       154\n",
            "           1       0.71      0.56      0.62       107\n",
            "           2       0.71      0.72      0.72       193\n",
            "           3       0.72      0.83      0.77       310\n",
            "           4       0.85      0.69      0.76       397\n",
            "           5       0.50      0.59      0.54        22\n",
            "           6       0.90      0.90      0.90       344\n",
            "           7       0.00      0.00      0.00         3\n",
            "           8       0.52      0.57      0.55        21\n",
            "           9       0.48      0.62      0.54        16\n",
            "          10       0.80      0.88      0.84       121\n",
            "          11       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.77      1688\n",
            "   macro avg       0.57      0.60      0.58      1688\n",
            "weighted avg       0.78      0.77      0.77      1688\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgeWI2RIY8mv",
        "outputId": "29f87b7d-0ffb-4957-bb5e-a738ea9d5a38"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svc=SVC(random_state=42)\n",
        "svc.fit(X_ros,y_ros)\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=42, shrinking=True, tol=0.001,\n",
              "    verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHXVst7sZDtX",
        "outputId": "436beeaf-255a-4745-b275-fe71b7c55a92"
      },
      "source": [
        "# Finding the accuracy score of model on test data\n",
        "nb_acc = svc.score(X_test, y_test)\n",
        "print(nb_acc)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7565165876777251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Zjm3UBmZLEH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}